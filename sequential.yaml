data_path: asset/data
dataset: ml-100k
field_separator: "\t"

USER_ID_FIELD: user_id
ITEM_ID_FIELD: item_id
TIME_FIELD: timestamp
MAX_ITEM_LIST_LENGTH: 50
user_inter_num_interval: [20, 100]

# Optional settings if you want to use implicit feedback
load_col:
    inter: [user_id, item_id, timestamp]

# training and evaluation
epochs: 200
train_batch_size: 128
eval_batch_size: 256
learning_rate: 0.001
learner: 'adam'
train_neg_sample_args: ~

eval_args:                      # (dict) 4 keys: group_by, order, split, and mode
  split: {'LK': ['test_only', 10]}   # (dict) The splitting strategy ranging in ['RS','LS'].{'RS':[0.8,0.1,0.1]}
  #split: { 'LS': 'valid_and_test' }   # (dict) The splitting strategy ranging in ['RS','LS'].{'RS':[0.8,0.1,0.1]}
  #split: { 'RS':[0.7,0.1,0.2] }   # (dict) The splitting strategy ranging in ['RS','LS'].{'RS':[0.8,0.1,0.1]}
  group_by: user                # (str) The grouping strategy ranging in ['user', 'none'].
  order: TO                     # (str) The ordering strategy ranging in ['RO', 'TO'].
  mode: uni100

# model
n_layers: 2                     # (int) The number of transformer layers in transformer encoder.
n_heads: 1                      # (int) The number of attention heads for multi-head attention layer.
hidden_size: 20                 # (int) The number of features in the hidden state.
inner_size: 20                  # (int) The inner hidden size in feed-forward layer.
hidden_dropout_prob: 0.2        # (float) The probability of an element to be zeroed.
attn_dropout_prob: 0.2          # (float) The probability of an attention score to be zeroed.
hidden_act: 'gelu'              # (str) The activation function in feed-forward layer.
layer_norm_eps: 1e-5            # (float) A value added to the denominator for numerical stability.
initializer_range: 0.02         # (float) The standard deviation for normal initialization.
loss_type: 'CE'                 # (str) The type of loss function. Range in ['BPR', 'CE'].
embedding_size: 64