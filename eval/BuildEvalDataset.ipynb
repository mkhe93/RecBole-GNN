{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:20.531214Z",
     "start_time": "2025-01-15T10:13:20.516609Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# setting proper working directory\n",
    "PROJECT_DIRECTORY = Path(os.path.abspath('')).resolve().parents[0]\n",
    "sys.path.extend([str(PROJECT_DIRECTORY)])\n",
    "\n",
    "print(f'Python {sys.version} on {sys.platform}')\n",
    "print('Project directory: ', PROJECT_DIRECTORY)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)] on win32\n",
      "Project directory:  C:\\Users\\s8347434\\Documents\\RecBole-GNN\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:11:21.607555Z",
     "start_time": "2025-01-15T13:11:21.601125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ast\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from recbole_gnn.config import Config\n",
    "from recbole_gnn.utils import create_dataset\n",
    "from recbole_gnn.data.dataset_metrics import GraphDatasetEvaluator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import statsmodels.formula.api as sm\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import gaussian_kde"
   ],
   "id": "53812e739772c602",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Read Evaluation Data\n",
    "- model evaluation from test runs\n",
    "- data set evaluation (topological characteristics)"
   ],
   "id": "d2885e57b1607b25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:23.484735Z",
     "start_time": "2025-01-15T10:13:23.450019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_split_characteristics_df = pd.read_csv(\"../eval/log/Dataset/dataset_eval.csv\", sep=\"\\t\")\n",
    "print(dataset_split_characteristics_df.shape)"
   ],
   "id": "dc4f2fcd9b14f8c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 37)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:25.655296Z",
     "start_time": "2025-01-15T10:13:25.638106Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_split_characteristics_df['density'] = 1 - dataset_split_characteristics_df['sparsity']",
   "id": "1d0918c8f7e2ca52",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:26.684424Z",
     "start_time": "2025-01-15T10:13:26.124018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EVAL_RUN = \"RO\"\n",
    "#als_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/ALS-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "asym_user_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/AsymKNNUser-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "asym_item_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/AsymKNNItem-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "#bpr_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/BPR-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "ngcf_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/NGCF-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "lightgcn_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/LightGCN-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "sgl_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/SGL-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "xsimgcl_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/XSimGCL-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")\n",
    "pop_df = pd.read_csv(f\"log/Benchmark/{EVAL_RUN}/Pop-Benchmark-{EVAL_RUN}.csv\", sep=\"\\t\")"
   ],
   "id": "8324a64f58f4db76",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:28.296838Z",
     "start_time": "2025-01-15T10:13:28.272950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_evaluation_df = pd.concat([pop_df, asym_user_df, asym_item_df, ngcf_df, lightgcn_df, sgl_df, xsimgcl_df], ignore_index=True)\n",
    "model_evaluation_df['dataset'] = model_evaluation_df['dataset'].str.extract(r'-(\\d+)$').astype(int)\n",
    "model_evaluation_df[\"eval_type\"] = [EVAL_RUN] * len(model_evaluation_df)\n",
    "print(model_evaluation_df.shape)"
   ],
   "id": "b551cdad078acd49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 20)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:29.367131Z",
     "start_time": "2025-01-15T10:13:29.356158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to process columns\n",
    "def process_user_columns(df):\n",
    "    for col in df.columns:\n",
    "        if col.startswith('best_user_') or col.startswith('worst_user_') or col.startswith('clustering_coefficients'):\n",
    "            # Apply transformation for each row in the selected columns\n",
    "            df[col] = df[col].apply(ast.literal_eval)            \n",
    "    return df"
   ],
   "id": "aab9cdba1614789d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:31.379332Z",
     "start_time": "2025-01-15T10:13:29.939317Z"
    }
   },
   "cell_type": "code",
   "source": "model_evaluation_df = process_user_columns(model_evaluation_df)",
   "id": "8dc3faedfa6bb3c6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:31.469106Z",
     "start_time": "2025-01-15T10:13:31.456312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation_dataset_characteristics_df = pd.merge(model_evaluation_df, dataset_split_characteristics_df, on='dataset', how='left')\n",
    "print(evaluation_dataset_characteristics_df.shape)"
   ],
   "id": "6438c5d69eb18152",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 57)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create necessary Data for Evaluation\n",
    "- classical characteristics per best / worst user (popularity, interactions)\n",
    "- topological characteristics per best / worst user\n",
    "- match userID with global userID"
   ],
   "id": "418f4ef468d2f377"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:33.653232Z",
     "start_time": "2025-01-15T10:13:33.643683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_users_topological_chars(data, file_path, num_datasets):    \n",
    "    \n",
    "    dataset_eval_list = []\n",
    "    for i in tqdm(range(num_datasets), total=num_datasets, unit='datasets'):        \n",
    "        # mapping recbole ID -> local ID (as recbole resets the index after filtering the datasets)\n",
    "        config = Config(model=\"BPR\", dataset=f\"real-life-atomic-100000-{i+1}\", config_file_list=[\"config_files/datasets.yaml\"])\n",
    "        dataset = create_dataset(config)\n",
    "\n",
    "        # Extract unique user IDs from best_user_ and worst_user_ columns\n",
    "        best_user_ids = set()  # Use a set to ensure uniqueness\n",
    "        worst_user_ids = set()  # Use a set to ensure uniqueness\n",
    "        for index, row in data[data['dataset']==i+1].iterrows():\n",
    "            for col in data.columns:\n",
    "                if col.startswith('best_user_'):\n",
    "                    # Extract user IDs from the list of dictionaries\n",
    "                    for entry in row[col]:  # Assuming each entry is a list of dictionaries\n",
    "                        best_user_ids.add(int(list(entry.keys())[0]))  # Add user ID (the key) to the set\n",
    "                if col.startswith('worst_user_'):\n",
    "                    # Extract user IDs from the list of dictionaries\n",
    "                    for entry in row[col]:  # Assuming each entry is a list of dictionaries\n",
    "                        worst_user_ids.add(int(list(entry.keys())[0]))  # Add user ID (the key) to the set\n",
    "\n",
    "        # calculate dataset metrics\n",
    "        dataset_evaluator = GraphDatasetEvaluator(config, dataset)\n",
    "        # some metrics need a connected graph and thus drops nodes which are in smallest partition\n",
    "        dataset_eval_dict = {\"dataset\": i+1}\n",
    "        dataset_eval_dict.update(dataset_evaluator.evaluate_best_worst_users(best_user_ids, worst_user_ids))\n",
    "        dataset_eval_list.append(dataset_eval_dict)\n",
    "\n",
    "        df = pd.DataFrame(dataset_eval_list)\n",
    "        df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "    return df"
   ],
   "id": "31ef512744de6185",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:36.858302Z",
     "start_time": "2025-01-15T10:13:36.833127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HINT: takes approx. 5h\n",
    "file_path = \"log/Dataset/user_topological_characteristics.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File does not exist, calculate all user's topology characteristics for each dataset..\")\n",
    "    user_topologies_df = get_users_topological_chars(evaluation_dataset_characteristics_df, file_path, num_datasets=177)\n",
    "else:\n",
    "    print(f\"File exists! Load file from {file_path}\")\n",
    "    user_topologies_df = pd.read_csv(file_path, sep='\\t')\n",
    "    user_topologies_df = process_user_columns(user_topologies_df)"
   ],
   "id": "10204b2805757da2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists! Load file from log/Dataset/user_topological_characteristics.csv\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:39.036041Z",
     "start_time": "2025-01-15T10:13:39.021144Z"
    }
   },
   "cell_type": "code",
   "source": "user_topologies_df",
   "id": "b325ea8e082aff72",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     dataset  degree_assort_best_users  degree_assort_worst_users  \\\n",
       "0          1                 -0.093766                   0.083721   \n",
       "1          2                 -0.079205                   0.104290   \n",
       "2          3                 -0.036276                   0.132727   \n",
       "3          4                 -0.057325                   0.018219   \n",
       "4          5                 -0.103245                  -0.025815   \n",
       "..       ...                       ...                        ...   \n",
       "172      173                 -0.109842                   0.126600   \n",
       "173      174                 -0.092176                   0.133401   \n",
       "174      175                 -0.065099                  -0.083633   \n",
       "175      176                 -0.091919                   0.061838   \n",
       "176      177                 -0.082224                   0.089979   \n",
       "\n",
       "     average_clustering_coef_dot_best_users  \\\n",
       "0                                  0.046294   \n",
       "1                                  0.057149   \n",
       "2                                  0.057840   \n",
       "3                                  0.042298   \n",
       "4                                  0.046922   \n",
       "..                                      ...   \n",
       "172                                0.052375   \n",
       "173                                0.048152   \n",
       "174                                0.041568   \n",
       "175                                0.057624   \n",
       "176                                0.052727   \n",
       "\n",
       "     average_clustering_coef_dot_worst_users  \n",
       "0                                   0.023263  \n",
       "1                                   0.025817  \n",
       "2                                   0.029270  \n",
       "3                                   0.023618  \n",
       "4                                   0.023108  \n",
       "..                                       ...  \n",
       "172                                 0.023652  \n",
       "173                                 0.026208  \n",
       "174                                 0.019093  \n",
       "175                                 0.022613  \n",
       "176                                 0.026730  \n",
       "\n",
       "[177 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>degree_assort_best_users</th>\n",
       "      <th>degree_assort_worst_users</th>\n",
       "      <th>average_clustering_coef_dot_best_users</th>\n",
       "      <th>average_clustering_coef_dot_worst_users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.093766</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.046294</td>\n",
       "      <td>0.023263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.079205</td>\n",
       "      <td>0.104290</td>\n",
       "      <td>0.057149</td>\n",
       "      <td>0.025817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.036276</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.029270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.057325</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.023618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.103245</td>\n",
       "      <td>-0.025815</td>\n",
       "      <td>0.046922</td>\n",
       "      <td>0.023108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>173</td>\n",
       "      <td>-0.109842</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.023652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>174</td>\n",
       "      <td>-0.092176</td>\n",
       "      <td>0.133401</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.026208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>175</td>\n",
       "      <td>-0.065099</td>\n",
       "      <td>-0.083633</td>\n",
       "      <td>0.041568</td>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>176</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.057624</td>\n",
       "      <td>0.022613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>177</td>\n",
       "      <td>-0.082224</td>\n",
       "      <td>0.089979</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.026730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge all together",
   "id": "c0b3bfe5a8768ee6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:53.219967Z",
     "start_time": "2025-01-15T10:13:53.205919Z"
    }
   },
   "cell_type": "code",
   "source": "evaluation_dataset_characteristics_user_topologies_df = pd.merge(evaluation_dataset_characteristics_df, user_topologies_df, on='dataset', how='left')  ",
   "id": "b1b501f1cf6ff19a",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:13:53.753523Z",
     "start_time": "2025-01-15T10:13:53.745440Z"
    }
   },
   "cell_type": "code",
   "source": "print(evaluation_dataset_characteristics_user_topologies_df.shape)",
   "id": "8e5bbce667b01a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 61)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Translate the userIDs into the userIDs out of the original dataset\n",
    "- global ID: the userID which holds through all splits\n",
    "- local ID: the userID which is only valid within one split\n",
    "- recbole ID: the userID which is assigned after the filtering"
   ],
   "id": "9ad9b23c4a07c748"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:14:51.194916Z",
     "start_time": "2025-01-15T10:14:51.186690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_userids(df, num_rows):      \n",
    "    # Extract columns ending with @[10]\n",
    "    data = df.copy()\n",
    "    columns_to_process = [col for col in data.columns if col.endswith(\"@[10]\")]\n",
    "    \n",
    "    for index, row in tqdm(data.iloc[:num_rows].iterrows(), total=num_rows, unit='rows'):\n",
    "        # configurations initialization\n",
    "        FILENAME = PROJECT_DIRECTORY / f\"asset/data/real-life-atomic-splits/real-life-atomic-100000-{row['dataset']}/real-life-atomic-100000-{row['dataset']}.inter\"\n",
    "        db = pd.read_csv(FILENAME, sep=\"\\t\", encoding=\"utf-8\")\n",
    "        \n",
    "        # mapping recbole ID -> local ID (as recbole resets the index after filtering the datasets)\n",
    "        config = Config(model=\"BPR\", dataset=f\"real-life-atomic-100000-{row['dataset']}\", config_file_list=[\"config_files/datasets.yaml\"])\n",
    "        dataset = create_dataset(config)\n",
    "        flipped_dict = {v: k for k, v in dataset.field2token_id['user_id'].items()}\n",
    "\n",
    "        # mapping local ID -> global ID\n",
    "        translation_dict = dict(zip(db[\"user_id:token\"], db[\"userID:token\"]))\n",
    "\n",
    "        # Process each column\n",
    "        for col in columns_to_process:\n",
    "            # Parse the string entries and extract the user IDs\n",
    "            #print(row['Model'], row['dataset'], col)\n",
    "            new_entry = {str(translation_dict[int(flipped_dict[int(list(entry.keys())[0])])]) : float(list(entry.values())[0]) for entry in row[col]}\n",
    "            data.at[index, col] = new_entry\n",
    "            \n",
    "    return data"
   ],
   "id": "b30f775f2c89431c",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T10:55:34.728470Z",
     "start_time": "2025-01-15T10:14:51.803372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HINT: takes approx. 20-30min.\n",
    "file_path = \"log/Benchmark/Overall-Benchmark-RO.csv\"\n",
    "file_path = \"log/Benchmark/Overall-Benchmark-RO-subset.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File does not exist, translate userIDs for each dataset..\")\n",
    "    translated_ids_df = translate_userids(evaluation_dataset_characteristics_user_topologies_df, num_rows=evaluation_dataset_characteristics_user_topologies_df.shape[0])\n",
    "    translated_ids_df.to_csv(file_path, sep='\\t', index=False)\n",
    "else:\n",
    "    print(f\"File exists! Load file from {file_path}\")\n",
    "    translated_ids_df = pd.read_csv(file_path, sep='\\t')\n",
    "    translated_ids_df = translated_ids_df[translated_ids_df['dataset'] != 177]\n",
    "    translated_ids_df = translated_ids_df.fillna({})\n",
    "    translated_ids_df = process_user_columns(translated_ids_df)"
   ],
   "id": "9b898f6994020fd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist, translate userIDs for each dataset..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1240/1240 [40:42<00:00,  1.97s/rows]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T11:23:49.070779Z",
     "start_time": "2025-01-15T11:22:06.661295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save DataFrame to a Parquet file\n",
    "file_path = \"../eval/log/Benchmark/translated_df.parquet\"\n",
    "\n",
    "translated_ids_df.to_parquet(file_path, engine=\"pyarrow\", index=False)"
   ],
   "id": "2e6cdc66aa9bbe09",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:08:09.954069Z",
     "start_time": "2025-01-15T13:05:35.643961Z"
    }
   },
   "cell_type": "code",
   "source": "loaded_df = pd.read_parquet(file_path, engine=\"pyarrow\")",
   "id": "ddcc0614e38b7aaf",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:17:39.551587Z",
     "start_time": "2025-01-15T13:17:39.525441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = translated_ids_df.copy()\n",
    "example['best_user_precision@[10]'] = example['best_user_precision@[10]'].apply(json.dumps)"
   ],
   "id": "89cdc923927b88f",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:21:47.095268Z",
     "start_time": "2025-01-15T13:17:45.141824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../eval/log/Benchmark/example_json.parquet\"\n",
    "example.to_parquet(file_path, engine=\"pyarrow\", index=False)\n",
    "example = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "example['best_user_precision@[10]'] = example['best_user_precision@[10]'].apply(json.loads)"
   ],
   "id": "55d9dd0e5eff5d0",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:08:49.833477Z",
     "start_time": "2025-01-15T14:08:49.770606Z"
    }
   },
   "cell_type": "code",
   "source": "example",
   "id": "c2b0b683d78b33d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Model  dataset  precision@10  hit@10  mrr@10  ndcg@10  map@10  \\\n",
       "0         Pop        1        0.0426  0.2676  0.1078   0.0513  0.0217   \n",
       "1         Pop        2        0.0313  0.2020  0.0806   0.0382  0.0162   \n",
       "2         Pop        3        0.0473  0.2945  0.1274   0.0636  0.0301   \n",
       "3         Pop        4        0.0528  0.3015  0.1404   0.0690  0.0334   \n",
       "4         Pop        5        0.0343  0.2308  0.0805   0.0407  0.0170   \n",
       "...       ...      ...           ...     ...     ...      ...     ...   \n",
       "1235  XSimGCL      173        0.1576  0.6140  0.3319   0.2030  0.1251   \n",
       "1236  XSimGCL      174        0.1567  0.6431  0.3480   0.2039  0.1208   \n",
       "1237  XSimGCL      175        0.1404  0.5764  0.3079   0.1796  0.1099   \n",
       "1238  XSimGCL      176        0.1472  0.5805  0.3085   0.1867  0.1152   \n",
       "1239  XSimGCL      177        0.1563  0.6252  0.3346   0.2045  0.1246   \n",
       "\n",
       "      itemcoverage@10  averagepopularity@10  tailpercentage@10  ...  \\\n",
       "0              0.0020              120.4951             0.0000  ...   \n",
       "1              0.0018               93.7833             0.0000  ...   \n",
       "2              0.0029              150.6464             0.0000  ...   \n",
       "3              0.0023              146.2831             0.0000  ...   \n",
       "4              0.0021              100.1591             0.0000  ...   \n",
       "...               ...                   ...                ...  ...   \n",
       "1235           0.1073               57.3223             0.0040  ...   \n",
       "1236           0.1160               67.9777             0.0052  ...   \n",
       "1237           0.1103               43.4651             0.0090  ...   \n",
       "1238           0.1085               58.8952             0.0095  ...   \n",
       "1239           0.1007               74.4384             0.0076  ...   \n",
       "\n",
       "     average_clustering_coef_min_item average_clustering_coef_max  \\\n",
       "0                            0.673293                    0.267778   \n",
       "1                            0.745253                    0.347611   \n",
       "2                            0.700724                    0.272593   \n",
       "3                            0.688358                    0.289853   \n",
       "4                            0.686097                    0.280193   \n",
       "...                               ...                         ...   \n",
       "1235                         0.725371                    0.318710   \n",
       "1236                         0.742498                    0.300826   \n",
       "1237                         0.711805                    0.286854   \n",
       "1238                         0.742387                    0.321873   \n",
       "1239                         0.764025                    0.339455   \n",
       "\n",
       "     average_clustering_coef_max_user average_clustering_coef_max_item  \\\n",
       "0                            0.042524                         0.279763   \n",
       "1                            0.052145                         0.362474   \n",
       "2                            0.058749                         0.289451   \n",
       "3                            0.043892                         0.303190   \n",
       "4                            0.045598                         0.292586   \n",
       "...                               ...                              ...   \n",
       "1235                         0.050125                         0.335126   \n",
       "1236                         0.051739                         0.319921   \n",
       "1237                         0.038564                         0.300418   \n",
       "1238                         0.050143                         0.337916   \n",
       "1239                         0.056421                         0.358764   \n",
       "\n",
       "       density degree_assort_best_users degree_assort_worst_users  \\\n",
       "0     0.004025                -0.093766                  0.083721   \n",
       "1     0.003342                -0.079205                  0.104290   \n",
       "2     0.003679                -0.036276                  0.132727   \n",
       "3     0.003699                -0.057325                  0.018219   \n",
       "4     0.003955                -0.103245                 -0.025815   \n",
       "...        ...                      ...                       ...   \n",
       "1235  0.003670                -0.109842                  0.126600   \n",
       "1236  0.003596                -0.092176                  0.133401   \n",
       "1237  0.003866                -0.065099                 -0.083633   \n",
       "1238  0.003431                -0.091919                  0.061838   \n",
       "1239  0.003438                -0.082224                  0.089979   \n",
       "\n",
       "     average_clustering_coef_dot_best_users  \\\n",
       "0                                  0.046294   \n",
       "1                                  0.057149   \n",
       "2                                  0.057840   \n",
       "3                                  0.042298   \n",
       "4                                  0.046922   \n",
       "...                                     ...   \n",
       "1235                               0.052375   \n",
       "1236                               0.048152   \n",
       "1237                               0.041568   \n",
       "1238                               0.057624   \n",
       "1239                               0.052727   \n",
       "\n",
       "     average_clustering_coef_dot_worst_users eval_type  \n",
       "0                                   0.023263        RO  \n",
       "1                                   0.025817        RO  \n",
       "2                                   0.029270        RO  \n",
       "3                                   0.023618        RO  \n",
       "4                                   0.023108        RO  \n",
       "...                                      ...       ...  \n",
       "1235                                0.023652        RO  \n",
       "1236                                0.026208        RO  \n",
       "1237                                0.019093        RO  \n",
       "1238                                0.022613        RO  \n",
       "1239                                0.026730        RO  \n",
       "\n",
       "[1240 rows x 62 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>hit@10</th>\n",
       "      <th>mrr@10</th>\n",
       "      <th>ndcg@10</th>\n",
       "      <th>map@10</th>\n",
       "      <th>itemcoverage@10</th>\n",
       "      <th>averagepopularity@10</th>\n",
       "      <th>tailpercentage@10</th>\n",
       "      <th>...</th>\n",
       "      <th>average_clustering_coef_min_item</th>\n",
       "      <th>average_clustering_coef_max</th>\n",
       "      <th>average_clustering_coef_max_user</th>\n",
       "      <th>average_clustering_coef_max_item</th>\n",
       "      <th>density</th>\n",
       "      <th>degree_assort_best_users</th>\n",
       "      <th>degree_assort_worst_users</th>\n",
       "      <th>average_clustering_coef_dot_best_users</th>\n",
       "      <th>average_clustering_coef_dot_worst_users</th>\n",
       "      <th>eval_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.2676</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>120.4951</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673293</td>\n",
       "      <td>0.267778</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.279763</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>-0.093766</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.046294</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pop</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>93.7833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.347611</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>0.362474</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>-0.079205</td>\n",
       "      <td>0.104290</td>\n",
       "      <td>0.057149</td>\n",
       "      <td>0.025817</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pop</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>150.6464</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700724</td>\n",
       "      <td>0.272593</td>\n",
       "      <td>0.058749</td>\n",
       "      <td>0.289451</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>-0.036276</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>146.2831</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688358</td>\n",
       "      <td>0.289853</td>\n",
       "      <td>0.043892</td>\n",
       "      <td>0.303190</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>-0.057325</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pop</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>100.1591</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686097</td>\n",
       "      <td>0.280193</td>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.292586</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>-0.103245</td>\n",
       "      <td>-0.025815</td>\n",
       "      <td>0.046922</td>\n",
       "      <td>0.023108</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>173</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>0.3319</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>57.3223</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725371</td>\n",
       "      <td>0.318710</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>-0.109842</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>174</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>0.6431</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.2039</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>67.9777</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742498</td>\n",
       "      <td>0.300826</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>0.319921</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>-0.092176</td>\n",
       "      <td>0.133401</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.026208</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>175</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.5764</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1103</td>\n",
       "      <td>43.4651</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711805</td>\n",
       "      <td>0.286854</td>\n",
       "      <td>0.038564</td>\n",
       "      <td>0.300418</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>-0.065099</td>\n",
       "      <td>-0.083633</td>\n",
       "      <td>0.041568</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>176</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>58.8952</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742387</td>\n",
       "      <td>0.321873</td>\n",
       "      <td>0.050143</td>\n",
       "      <td>0.337916</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.057624</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>177</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>0.3346</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>74.4384</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764025</td>\n",
       "      <td>0.339455</td>\n",
       "      <td>0.056421</td>\n",
       "      <td>0.358764</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>-0.082224</td>\n",
       "      <td>0.089979</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 62 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T13:14:08.532500Z",
     "start_time": "2025-01-15T13:14:08.485677Z"
    }
   },
   "cell_type": "code",
   "source": "translated_ids_df",
   "id": "72910657afc78f69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Model  dataset  precision@10  hit@10  mrr@10  ndcg@10  map@10  \\\n",
       "0         Pop        1        0.0426  0.2676  0.1078   0.0513  0.0217   \n",
       "1         Pop        2        0.0313  0.2020  0.0806   0.0382  0.0162   \n",
       "2         Pop        3        0.0473  0.2945  0.1274   0.0636  0.0301   \n",
       "3         Pop        4        0.0528  0.3015  0.1404   0.0690  0.0334   \n",
       "4         Pop        5        0.0343  0.2308  0.0805   0.0407  0.0170   \n",
       "...       ...      ...           ...     ...     ...      ...     ...   \n",
       "1235  XSimGCL      173        0.1576  0.6140  0.3319   0.2030  0.1251   \n",
       "1236  XSimGCL      174        0.1567  0.6431  0.3480   0.2039  0.1208   \n",
       "1237  XSimGCL      175        0.1404  0.5764  0.3079   0.1796  0.1099   \n",
       "1238  XSimGCL      176        0.1472  0.5805  0.3085   0.1867  0.1152   \n",
       "1239  XSimGCL      177        0.1563  0.6252  0.3346   0.2045  0.1246   \n",
       "\n",
       "      itemcoverage@10  averagepopularity@10  tailpercentage@10  ...  \\\n",
       "0              0.0020              120.4951             0.0000  ...   \n",
       "1              0.0018               93.7833             0.0000  ...   \n",
       "2              0.0029              150.6464             0.0000  ...   \n",
       "3              0.0023              146.2831             0.0000  ...   \n",
       "4              0.0021              100.1591             0.0000  ...   \n",
       "...               ...                   ...                ...  ...   \n",
       "1235           0.1073               57.3223             0.0040  ...   \n",
       "1236           0.1160               67.9777             0.0052  ...   \n",
       "1237           0.1103               43.4651             0.0090  ...   \n",
       "1238           0.1085               58.8952             0.0095  ...   \n",
       "1239           0.1007               74.4384             0.0076  ...   \n",
       "\n",
       "     average_clustering_coef_min_item average_clustering_coef_max  \\\n",
       "0                            0.673293                    0.267778   \n",
       "1                            0.745253                    0.347611   \n",
       "2                            0.700724                    0.272593   \n",
       "3                            0.688358                    0.289853   \n",
       "4                            0.686097                    0.280193   \n",
       "...                               ...                         ...   \n",
       "1235                         0.725371                    0.318710   \n",
       "1236                         0.742498                    0.300826   \n",
       "1237                         0.711805                    0.286854   \n",
       "1238                         0.742387                    0.321873   \n",
       "1239                         0.764025                    0.339455   \n",
       "\n",
       "     average_clustering_coef_max_user average_clustering_coef_max_item  \\\n",
       "0                            0.042524                         0.279763   \n",
       "1                            0.052145                         0.362474   \n",
       "2                            0.058749                         0.289451   \n",
       "3                            0.043892                         0.303190   \n",
       "4                            0.045598                         0.292586   \n",
       "...                               ...                              ...   \n",
       "1235                         0.050125                         0.335126   \n",
       "1236                         0.051739                         0.319921   \n",
       "1237                         0.038564                         0.300418   \n",
       "1238                         0.050143                         0.337916   \n",
       "1239                         0.056421                         0.358764   \n",
       "\n",
       "       density degree_assort_best_users degree_assort_worst_users  \\\n",
       "0     0.004025                -0.093766                  0.083721   \n",
       "1     0.003342                -0.079205                  0.104290   \n",
       "2     0.003679                -0.036276                  0.132727   \n",
       "3     0.003699                -0.057325                  0.018219   \n",
       "4     0.003955                -0.103245                 -0.025815   \n",
       "...        ...                      ...                       ...   \n",
       "1235  0.003670                -0.109842                  0.126600   \n",
       "1236  0.003596                -0.092176                  0.133401   \n",
       "1237  0.003866                -0.065099                 -0.083633   \n",
       "1238  0.003431                -0.091919                  0.061838   \n",
       "1239  0.003438                -0.082224                  0.089979   \n",
       "\n",
       "     average_clustering_coef_dot_best_users  \\\n",
       "0                                  0.046294   \n",
       "1                                  0.057149   \n",
       "2                                  0.057840   \n",
       "3                                  0.042298   \n",
       "4                                  0.046922   \n",
       "...                                     ...   \n",
       "1235                               0.052375   \n",
       "1236                               0.048152   \n",
       "1237                               0.041568   \n",
       "1238                               0.057624   \n",
       "1239                               0.052727   \n",
       "\n",
       "     average_clustering_coef_dot_worst_users eval_type  \n",
       "0                                   0.023263        RO  \n",
       "1                                   0.025817        RO  \n",
       "2                                   0.029270        RO  \n",
       "3                                   0.023618        RO  \n",
       "4                                   0.023108        RO  \n",
       "...                                      ...       ...  \n",
       "1235                                0.023652        RO  \n",
       "1236                                0.026208        RO  \n",
       "1237                                0.019093        RO  \n",
       "1238                                0.022613        RO  \n",
       "1239                                0.026730        RO  \n",
       "\n",
       "[1240 rows x 62 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>precision@10</th>\n",
       "      <th>hit@10</th>\n",
       "      <th>mrr@10</th>\n",
       "      <th>ndcg@10</th>\n",
       "      <th>map@10</th>\n",
       "      <th>itemcoverage@10</th>\n",
       "      <th>averagepopularity@10</th>\n",
       "      <th>tailpercentage@10</th>\n",
       "      <th>...</th>\n",
       "      <th>average_clustering_coef_min_item</th>\n",
       "      <th>average_clustering_coef_max</th>\n",
       "      <th>average_clustering_coef_max_user</th>\n",
       "      <th>average_clustering_coef_max_item</th>\n",
       "      <th>density</th>\n",
       "      <th>degree_assort_best_users</th>\n",
       "      <th>degree_assort_worst_users</th>\n",
       "      <th>average_clustering_coef_dot_best_users</th>\n",
       "      <th>average_clustering_coef_dot_worst_users</th>\n",
       "      <th>eval_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pop</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.2676</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>120.4951</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673293</td>\n",
       "      <td>0.267778</td>\n",
       "      <td>0.042524</td>\n",
       "      <td>0.279763</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>-0.093766</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>0.046294</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pop</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>93.7833</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.347611</td>\n",
       "      <td>0.052145</td>\n",
       "      <td>0.362474</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>-0.079205</td>\n",
       "      <td>0.104290</td>\n",
       "      <td>0.057149</td>\n",
       "      <td>0.025817</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pop</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>150.6464</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700724</td>\n",
       "      <td>0.272593</td>\n",
       "      <td>0.058749</td>\n",
       "      <td>0.289451</td>\n",
       "      <td>0.003679</td>\n",
       "      <td>-0.036276</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>0.057840</td>\n",
       "      <td>0.029270</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pop</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>146.2831</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688358</td>\n",
       "      <td>0.289853</td>\n",
       "      <td>0.043892</td>\n",
       "      <td>0.303190</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>-0.057325</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.023618</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pop</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.0805</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>100.1591</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686097</td>\n",
       "      <td>0.280193</td>\n",
       "      <td>0.045598</td>\n",
       "      <td>0.292586</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>-0.103245</td>\n",
       "      <td>-0.025815</td>\n",
       "      <td>0.046922</td>\n",
       "      <td>0.023108</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>173</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>0.3319</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.1073</td>\n",
       "      <td>57.3223</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725371</td>\n",
       "      <td>0.318710</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>0.335126</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>-0.109842</td>\n",
       "      <td>0.126600</td>\n",
       "      <td>0.052375</td>\n",
       "      <td>0.023652</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>174</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>0.6431</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.2039</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>67.9777</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742498</td>\n",
       "      <td>0.300826</td>\n",
       "      <td>0.051739</td>\n",
       "      <td>0.319921</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>-0.092176</td>\n",
       "      <td>0.133401</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>0.026208</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>175</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.5764</td>\n",
       "      <td>0.3079</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1103</td>\n",
       "      <td>43.4651</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711805</td>\n",
       "      <td>0.286854</td>\n",
       "      <td>0.038564</td>\n",
       "      <td>0.300418</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>-0.065099</td>\n",
       "      <td>-0.083633</td>\n",
       "      <td>0.041568</td>\n",
       "      <td>0.019093</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>176</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.1152</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>58.8952</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742387</td>\n",
       "      <td>0.321873</td>\n",
       "      <td>0.050143</td>\n",
       "      <td>0.337916</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>0.061838</td>\n",
       "      <td>0.057624</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>XSimGCL</td>\n",
       "      <td>177</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>0.3346</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>0.1007</td>\n",
       "      <td>74.4384</td>\n",
       "      <td>0.0076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764025</td>\n",
       "      <td>0.339455</td>\n",
       "      <td>0.056421</td>\n",
       "      <td>0.358764</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>-0.082224</td>\n",
       "      <td>0.089979</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.026730</td>\n",
       "      <td>RO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1240 rows × 62 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:39:04.932730Z",
     "start_time": "2025-01-12T19:39:04.925300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reformat_list_to_dicts(data, num_rows):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    columns_to_process = [col for col in df.columns if col.endswith(\"@[10]\")]\n",
    "    \n",
    "    for index, row in tqdm(df.iloc[:num_rows].iterrows(), total=num_rows, unit='rows'):\n",
    "        # Process each column\n",
    "        for col in columns_to_process:\n",
    "            # Parse the string entries and extract the user IDs\n",
    "            #print(row['Model'], row['dataset'], col)\n",
    "            print([entry for entry in row[col].values()])\n",
    "            new_entry = {str(list(entry.keys())[0]) : float(list(entry.values())[0]) for entry in row[col]}\n",
    "            \n",
    "            df.at[index, col] = new_entry\n",
    "            \n",
    "    return df"
   ],
   "id": "b02135ebb7f799ff",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:39:05.365794Z",
     "start_time": "2025-01-12T19:39:05.309847Z"
    }
   },
   "cell_type": "code",
   "source": "df = reformat_list_to_dicts(translated_ids_df, num_rows=len(translated_ids_df))",
   "id": "ea0debc6426bfae3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1594 [00:00<?, ?rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8, 0.5, 0.5, 0.5, 0.5, 0.5, 0.4, 0.4, 0.4, 0.4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[73], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mreformat_list_to_dicts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtranslated_ids_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtranslated_ids_df\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[72], line 13\u001B[0m, in \u001B[0;36mreformat_list_to_dicts\u001B[1;34m(data, num_rows)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m columns_to_process:\n\u001B[0;32m     10\u001B[0m         \u001B[38;5;66;03m# Parse the string entries and extract the user IDs\u001B[39;00m\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;66;03m#print(row['Model'], row['dataset'], col)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;28mprint\u001B[39m([entry \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m row[col]\u001B[38;5;241m.\u001B[39mvalues()])\n\u001B[1;32m---> 13\u001B[0m         new_entry \u001B[38;5;241m=\u001B[39m {\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mlist\u001B[39m(\u001B[43mentry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m())[\u001B[38;5;241m0\u001B[39m]) : \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mlist\u001B[39m(entry\u001B[38;5;241m.\u001B[39mvalues())[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m entry \u001B[38;5;129;01min\u001B[39;00m row[col]}\n\u001B[0;32m     15\u001B[0m         df\u001B[38;5;241m.\u001B[39mat[index, col] \u001B[38;5;241m=\u001B[39m new_entry\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'int' object has no attribute 'keys'"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:35:10.035699Z",
     "start_time": "2025-01-12T19:35:09.080739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save DataFrame to a Parquet file\n",
    "file_path = \"../eval/log/Benchmark/translated_df.parquet\"\n",
    "\n",
    "df.to_parquet(file_path, engine=\"pyarrow\", index=False)"
   ],
   "id": "71b68164f1bda12c",
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "(\"Expected dict key of type str or bytes, got 'int'\", 'Conversion failed for column best_user_precision@[10] with type object')",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mArrowTypeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[57], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Save DataFrame to a Parquet file\u001B[39;00m\n\u001B[0;32m      2\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../eval/log/Benchmark/translated_df.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpyarrow\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    328\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    329\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    330\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    331\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    332\u001B[0m     )\n\u001B[1;32m--> 333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:3113\u001B[0m, in \u001B[0;36mDataFrame.to_parquet\u001B[1;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001B[0m\n\u001B[0;32m   3032\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3033\u001B[0m \u001B[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001B[39;00m\n\u001B[0;32m   3034\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3109\u001B[0m \u001B[38;5;124;03m>>> content = f.read()\u001B[39;00m\n\u001B[0;32m   3110\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3111\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparquet\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_parquet\n\u001B[1;32m-> 3113\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mto_parquet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3114\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3119\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartition_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3121\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3122\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:480\u001B[0m, in \u001B[0;36mto_parquet\u001B[1;34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001B[0m\n\u001B[0;32m    476\u001B[0m impl \u001B[38;5;241m=\u001B[39m get_engine(engine)\n\u001B[0;32m    478\u001B[0m path_or_buf: FilePath \u001B[38;5;241m|\u001B[39m WriteBuffer[\u001B[38;5;28mbytes\u001B[39m] \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mBytesIO() \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m path\n\u001B[1;32m--> 480\u001B[0m \u001B[43mimpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpartition_cols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartition_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    486\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    492\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, io\u001B[38;5;241m.\u001B[39mBytesIO)\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pandas\\io\\parquet.py:190\u001B[0m, in \u001B[0;36mPyArrowImpl.write\u001B[1;34m(self, df, path, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    188\u001B[0m     from_pandas_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpreserve_index\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m index\n\u001B[1;32m--> 190\u001B[0m table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pandas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfrom_pandas_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df\u001B[38;5;241m.\u001B[39mattrs:\n\u001B[0;32m    193\u001B[0m     df_metadata \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPANDAS_ATTRS\u001B[39m\u001B[38;5;124m\"\u001B[39m: json\u001B[38;5;241m.\u001B[39mdumps(df\u001B[38;5;241m.\u001B[39mattrs)}\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\table.pxi:4751\u001B[0m, in \u001B[0;36mpyarrow.lib.Table.from_pandas\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:625\u001B[0m, in \u001B[0;36mdataframe_to_arrays\u001B[1;34m(df, schema, preserve_index, nthreads, columns, safe)\u001B[0m\n\u001B[0;32m    620\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    621\u001B[0m             arr\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mcontiguous \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    622\u001B[0m             \u001B[38;5;28missubclass\u001B[39m(arr\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mtype, np\u001B[38;5;241m.\u001B[39minteger))\n\u001B[0;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nthreads \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 625\u001B[0m     arrays \u001B[38;5;241m=\u001B[39m [\u001B[43mconvert_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    626\u001B[0m               \u001B[38;5;28;01mfor\u001B[39;00m c, f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(columns_to_convert, convert_fields)]\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     arrays \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:612\u001B[0m, in \u001B[0;36mdataframe_to_arrays.<locals>.convert_column\u001B[1;34m(col, field)\u001B[0m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (pa\u001B[38;5;241m.\u001B[39mArrowInvalid,\n\u001B[0;32m    608\u001B[0m         pa\u001B[38;5;241m.\u001B[39mArrowNotImplementedError,\n\u001B[0;32m    609\u001B[0m         pa\u001B[38;5;241m.\u001B[39mArrowTypeError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    610\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion failed for column \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m with type \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    611\u001B[0m                \u001B[38;5;241m.\u001B[39mformat(col\u001B[38;5;241m.\u001B[39mname, col\u001B[38;5;241m.\u001B[39mdtype),)\n\u001B[1;32m--> 612\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m field_nullable \u001B[38;5;129;01mand\u001B[39;00m result\u001B[38;5;241m.\u001B[39mnull_count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mField \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m was non-nullable but pandas column \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    615\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhad \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m null values\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mstr\u001B[39m(field),\n\u001B[0;32m    616\u001B[0m                                                  result\u001B[38;5;241m.\u001B[39mnull_count))\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\pandas_compat.py:606\u001B[0m, in \u001B[0;36mdataframe_to_arrays.<locals>.convert_column\u001B[1;34m(col, field)\u001B[0m\n\u001B[0;32m    603\u001B[0m     type_ \u001B[38;5;241m=\u001B[39m field\u001B[38;5;241m.\u001B[39mtype\n\u001B[0;32m    605\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 606\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mpa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtype_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_pandas\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msafe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msafe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (pa\u001B[38;5;241m.\u001B[39mArrowInvalid,\n\u001B[0;32m    608\u001B[0m         pa\u001B[38;5;241m.\u001B[39mArrowNotImplementedError,\n\u001B[0;32m    609\u001B[0m         pa\u001B[38;5;241m.\u001B[39mArrowTypeError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    610\u001B[0m     e\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConversion failed for column \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m with type \u001B[39m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    611\u001B[0m                \u001B[38;5;241m.\u001B[39mformat(col\u001B[38;5;241m.\u001B[39mname, col\u001B[38;5;241m.\u001B[39mdtype),)\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\array.pxi:360\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\array.pxi:87\u001B[0m, in \u001B[0;36mpyarrow.lib._ndarray_to_array\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\RecBole-GNN\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mArrowTypeError\u001B[0m: (\"Expected dict key of type str or bytes, got 'int'\", 'Conversion failed for column best_user_precision@[10] with type object')"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculate the popularity of each user's interactions",
   "id": "cb1ec7863ef67d44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:49:45.935485Z",
     "start_time": "2025-01-12T19:49:45.920012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_all_users_popularity(num_datasets):    \n",
    "    \n",
    "    # Initialize DataFrame with pre-defined size and columns\n",
    "    df = pd.DataFrame({'dataset': range(1, num_datasets + 1)})        \n",
    "    df['user_popularity'] = [{} for _ in range(num_datasets)]     \n",
    "    \n",
    "    # Loop through each dataset and compute user popularity\n",
    "    for i in tqdm(range(num_datasets)):\n",
    "        FILENAME = PROJECT_DIRECTORY / f\"asset/data/real-life-atomic-splits/real-life-atomic-100000-{i+1}/real-life-atomic-100000-{i+1}.inter\"\n",
    "        db = pd.read_csv(FILENAME, sep=\"\\t\", encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter users with less than 20 interactions\n",
    "        db['interaction_count'] = db.groupby(\"userID:token\")[\"itemID:token\"].transform('count')\n",
    "        filtered_df = db[db['interaction_count'] >= 20].copy()\n",
    "        \n",
    "        # Calculate item popularity\n",
    "        item_popularity = filtered_df.groupby(\"itemID:token\")[\"userID:token\"].nunique()\n",
    "        \n",
    "        # Map item popularity back to filtered_df safely\n",
    "        filtered_df.loc[:, 'item_popularity'] = filtered_df[\"itemID:token\"].map(item_popularity)\n",
    "        \n",
    "        # Compute user average popularity\n",
    "        user_avg_popularity = filtered_df.groupby(\"userID:token\")['item_popularity'].agg(['mean', 'median'])\n",
    "        \n",
    "        # Compute user popularity dictionary\n",
    "        all_users_popularity_dict = {\n",
    "            user_id: (row['mean'], row['median']) \n",
    "            for user_id, row in user_avg_popularity.iterrows()\n",
    "        }\n",
    "        \n",
    "        # Assign the dictionary to the DataFrame\n",
    "        df.at[i,'user_popularity'] = all_users_popularity_dict\n",
    "    \n",
    "    return df"
   ],
   "id": "c42b56abbeeacd8",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:49:56.633689Z",
     "start_time": "2025-01-12T19:49:52.014885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HINT: takes approx. \n",
    "file_path = \"../asset/data/real-life-atomic-splits/user_popularity.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File does not exist, calculate all user's popularity for each dataset..\")\n",
    "    popularity_dict = get_all_users_popularity(num_datasets=177)\n",
    "    popularity_dict.to_csv(file_path, sep='\\t', index=False)\n",
    "else:\n",
    "    print(f\"File exists! Load file from {file_path}\")\n",
    "    # Load the existing DataFrame\n",
    "    popularity_dict = pd.read_csv(file_path, sep='\\t') \n",
    "    # Convert the user_popularity column back to dictionaries\n",
    "    if 'user_popularity' in popularity_dict.columns:\n",
    "        popularity_dict['user_popularity'] = popularity_dict['user_popularity'].apply(ast.literal_eval) "
   ],
   "id": "d06882c8d5b7747b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists! Load file from ../asset/data/real-life-atomic-splits/user_popularity.csv\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:49:56.704177Z",
     "start_time": "2025-01-12T19:49:56.641689Z"
    }
   },
   "cell_type": "code",
   "source": "print(popularity_dict.head())",
   "id": "2f72da91ce86650d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dataset                                    user_popularity\n",
      "0        1  {1: (9.698447893569845, 4.0), 2: (3.44, 2.0), ...\n",
      "1        2  {1: (21.978021978021978, 3.0), 2: (23.41304347...\n",
      "2        3  {1: (16.871382636655948, 3.0), 2: (16.64761904...\n",
      "3        4  {1: (7.957446808510638, 4.0), 5: (5.28125, 4.0...\n",
      "4        5  {1: (34.275, 4.0), 4: (11.0, 3.0), 5: (22.3975...\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:49:57.772096Z",
     "start_time": "2025-01-12T19:49:57.736846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_user_popularity(data, all_users_popularity_dict, num_rows):\n",
    "    \n",
    "    df = data.copy()\n",
    "    \n",
    "    # Initialize new columns with empty lists\n",
    "    df['best_users_mean_popularity_dict'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_mean_popularity_mean'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_mean_popularity_max'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_mean_popularity_min'] = [{} for _ in range(len(df))]    \n",
    "    df['best_users_median_popularity_dict'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_median_popularity_mean'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_median_popularity_max'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_median_popularity_min'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_node_degree'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_node_degree_mean'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_node_degree_median'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_node_degree_min'] = [{} for _ in range(len(df))]\n",
    "    df['best_users_node_degree_max'] = [{} for _ in range(len(df))]\n",
    "\n",
    "    df['worst_users_mean_popularity_dict'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_mean_popularity_mean'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_mean_popularity_max'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_mean_popularity_min'] = [{} for _ in range(len(df))]    \n",
    "    df['worst_users_median_popularity_dict'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_median_popularity_mean'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_median_popularity_max'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_median_popularity_min'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_node_degree'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_node_degree_mean'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_node_degree_median'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_node_degree_min'] = [{} for _ in range(len(df))]\n",
    "    df['worst_users_node_degree_max'] = [{} for _ in range(len(df))]\n",
    "\n",
    "    df['all_users_median_popularity_mean'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_median_popularity_max'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_median_popularity_min'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_node_degree_mean'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_median_node_degree_median'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_node_degree_max'] = [{} for _ in range(len(df))]\n",
    "    df['all_users_node_degree_min'] = [{} for _ in range(len(df))]    \n",
    "     \n",
    "    # Loop through each row\n",
    "    for index, row in tqdm(df.iloc[:num_rows].iterrows(), total=num_rows, unit='rows'):\n",
    "\n",
    "        # configurations initialization\n",
    "        FILENAME = PROJECT_DIRECTORY / f\"asset/data/real-life-atomic-splits/real-life-atomic-100000-{row['dataset']}/real-life-atomic-100000-{row['dataset']}.inter\"\n",
    "        db = pd.read_csv(FILENAME, sep=\"\\t\", encoding=\"utf-8\")\n",
    "        \n",
    "        # Filter users with less than 20 node_degree\n",
    "        db['interaction_count'] = db.groupby(\"userID:token\")[\"itemID:token\"].transform('count')\n",
    "        filtered_df = db[db['interaction_count'] >= 20].copy()\n",
    "        all_users_node_degree_dict = filtered_df.groupby(\"userID:token\")['interaction_count'].first()\n",
    "                        \n",
    "        # Extract unique user IDs from best_user_ and worst_user_ columns\n",
    "        best_user_ids = set()  # Use a set to ensure uniqueness\n",
    "        worst_user_ids = set()  # Use a set to ensure uniqueness\n",
    "        for col in df.columns:\n",
    "            if col.startswith('best_user_'):\n",
    "                # Extract user IDs from the list of dictionaries\n",
    "                for entry in row[col]:  # Assuming each entry is a list of dictionaries\n",
    "                    best_user_ids.add(int(list(entry.keys())[0]))  # Add user ID (the key) to the set\n",
    "            if col.startswith('worst_user_'):\n",
    "                # Extract user IDs from the list of dictionaries\n",
    "                for entry in row[col]:  # Assuming each entry is a list of dictionaries\n",
    "                    worst_user_ids.add(int(list(entry.keys())[0]))  # Add user ID (the key) to the set\n",
    "                                         \n",
    "        # Prepare best users\n",
    "        best_users_mean_popularity_dict = {user_id: all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'][user_id][0] for user_id in best_user_ids}\n",
    "        best_users_median_popularity_dict = {user_id: all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'][user_id][1] for user_id in best_user_ids}\n",
    "        best_users_node_degree_dict = {user_id: all_users_node_degree_dict[user_id] for user_id in best_user_ids}     \n",
    "        best_users_mean_values_list = list(best_users_mean_popularity_dict.values())\n",
    "        best_users_median_values_list = list(best_users_median_popularity_dict.values())\n",
    "        best_users_node_degree_list = list(best_users_node_degree_dict.values())\n",
    "\n",
    "        worst_users_mean_popularity_dict = {user_id: all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'][user_id][0] for user_id in worst_user_ids}\n",
    "        worst_users_median_popularity_dict = {user_id: all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'][user_id][1] for user_id in worst_user_ids}\n",
    "        worst_users_node_degree_dict = {user_id: all_users_node_degree_dict[user_id] for user_id in worst_user_ids}     \n",
    "        worst_users_mean_values_list = list(worst_users_mean_popularity_dict.values())\n",
    "        worst_users_median_values_list = list(worst_users_median_popularity_dict.values())\n",
    "        worst_users_node_degree_list = list(worst_users_node_degree_dict.values())\n",
    "        \n",
    "        all_users_mean_values_list = [entry[0] for entry in list(all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'].values())]\n",
    "        all_users_median_values_list = [entry[1] for entry in list(all_users_popularity_dict.loc[row['dataset']-1,'user_popularity'].values())]\n",
    "\n",
    "        # Assign\n",
    "        df.at[index, 'best_users_mean_popularity_dict'] = best_users_mean_popularity_dict\n",
    "        df.at[index, 'best_users_mean_popularity_mean'] = np.mean(best_users_mean_values_list)\n",
    "        df.at[index, 'best_users_mean_popularity_max'] = np.max(best_users_mean_values_list)\n",
    "        df.at[index, 'best_users_mean_popularity_min'] = np.min(best_users_mean_values_list)\n",
    "        df.at[index, 'best_users_median_popularity_dict'] = best_users_median_popularity_dict\n",
    "        df.at[index, 'best_users_median_popularity_mean'] = np.mean(best_users_median_values_list)\n",
    "        df.at[index, 'best_users_median_popularity_max'] = np.max(best_users_median_values_list)\n",
    "        df.at[index, 'best_users_median_popularity_min'] = np.min(best_users_median_values_list)\n",
    "        df.at[index, 'best_users_node_degree'] = best_users_node_degree_dict\n",
    "        df.at[index, 'best_users_node_degree_mean'] = np.mean(best_users_node_degree_list)\n",
    "        df.at[index, 'best_users_node_degree_median'] = np.median(best_users_node_degree_list)\n",
    "        df.at[index, 'best_users_node_degree_min'] = np.min(best_users_node_degree_list)\n",
    "        df.at[index, 'best_users_node_degree_max'] = np.max(best_users_node_degree_list)\n",
    "\n",
    "        df.at[index, 'worst_users_mean_popularity_dict'] = worst_users_mean_popularity_dict\n",
    "        df.at[index, 'worst_users_mean_popularity_mean'] = np.mean(worst_users_mean_values_list)\n",
    "        df.at[index, 'worst_users_mean_popularity_max'] = np.max(worst_users_mean_values_list)\n",
    "        df.at[index, 'worst_users_mean_popularity_min'] = np.min(worst_users_mean_values_list)\n",
    "        df.at[index, 'worst_users_median_popularity_dict'] = worst_users_median_popularity_dict\n",
    "        df.at[index, 'worst_users_median_popularity_mean'] = np.mean(worst_users_median_values_list)\n",
    "        df.at[index, 'worst_users_median_popularity_max'] = np.max(worst_users_median_values_list)\n",
    "        df.at[index, 'worst_users_median_popularity_min'] = np.min(worst_users_median_values_list)\n",
    "        df.at[index, 'worst_users_node_degree'] = worst_users_node_degree_dict\n",
    "        df.at[index, 'worst_users_node_degree_mean'] = np.mean(worst_users_node_degree_list)\n",
    "        df.at[index, 'worst_users_node_degree_median'] = np.median(worst_users_node_degree_list)\n",
    "        df.at[index, 'worst_users_node_degree_min'] = np.min(worst_users_node_degree_list)\n",
    "        df.at[index, 'worst_users_node_degree_max'] = np.max(worst_users_node_degree_list)\n",
    "\n",
    "        df.at[index, 'all_users_mean_popularity_mean'] = np.mean(all_users_mean_values_list)\n",
    "        df.at[index, 'all_users_mean_popularity_max'] = np.max(all_users_mean_values_list)\n",
    "        df.at[index, 'all_users_mean_popularity_min'] = np.min(all_users_mean_values_list)\n",
    "        df.at[index, 'all_users_median_popularity_mean'] = np.mean(all_users_median_values_list)\n",
    "        df.at[index, 'all_users_median_popularity_max'] = np.max(all_users_median_values_list)\n",
    "        df.at[index, 'all_users_median_popularity_min'] = np.min(all_users_median_values_list)\n",
    "        df.at[index, 'all_users_node_degree_mean'] = all_users_node_degree_dict.mean()\n",
    "        df.at[index, 'all_users_median_node_degree_median'] = all_users_node_degree_dict.median()\n",
    "        df.at[index, 'all_users_node_degree_max'] = all_users_node_degree_dict.max()\n",
    "        df.at[index, 'all_users_node_degree_min'] = all_users_node_degree_dict.min()\n",
    "        \n",
    "    return df"
   ],
   "id": "8a4635b64c6d5d73",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:50:03.480590Z",
     "start_time": "2025-01-12T19:50:01.566183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE: takes approx. 2-3 min\n",
    "file_path = \"log/Dataset/user_classical_characteristics.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File does not exist, assign all user's popularity to each dataset..\")\n",
    "    classical_user_characteristics_df = get_user_popularity(translated_ids_df, popularity_dict, num_rows = translated_ids_df.shape[0])\n",
    "    classical_user_characteristics_df.to_csv(file_path, sep='\\t', index=False)\n",
    "else:\n",
    "    print(f\"File exists! Load file from {file_path}\")\n",
    "    classical_user_characteristics_df = pd.read_csv(file_path, sep='\\t')\n",
    "    classical_user_characteristics_df = process_user_columns(classical_user_characteristics_df)"
   ],
   "id": "c09e7895f1bb2b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists! Load file from log/Dataset/user_classical_characteristics.csv\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T19:51:17.790897Z",
     "start_time": "2025-01-12T19:50:14.437190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"../eval/log/Benchmark/TotalEvaluationData-RO.csv\"\n",
    "final_eval_df = pd.merge(translated_ids_df, classical_user_characteristics_df, on='dataset', how='left')\n",
    "final_eval_df = final_eval_df[final_eval_df['dataset'] != 177]\n",
    "final_eval_df.to_csv(file_path, sep='\\t', index=False)"
   ],
   "id": "e1ec97835e4127ae",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "file_path = \"../eval/log/Benchmark/TotalEvaluationData-RO.csv\"\n",
    "if not os.path.isfile(file_path):\n",
    "    print(\"File does not exist, please run all the lines above..\")\n",
    "else:\n",
    "    print(f\"File exists! Load file from {file_path}\")\n",
    "    # Load the existing DataFrame\n",
    "    final_eval_df = pd.read_csv(file_path, sep='\\t') \n",
    "    final_eval_df = process_user_columns(final_eval_df)\n",
    "    \n",
    "    # Convert the user_popularity column back to dictionaries\n",
    "    if 'user_popularity' in final_eval_df.columns:\n",
    "        final_eval_df['user_popularity'] = final_eval_df['user_popularity'].apply(ast.literal_eval) "
   ],
   "id": "8f84c00fd4215a4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save DataFrame to a Parquet file\n",
    "file_path = \"../eval/log/Benchmark/data.parquet\"\n",
    "\n",
    "final_eval_df.to_parquet(file_path, engine=\"pyarrow\", index=False)"
   ],
   "id": "3c7d8070b9fd55ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the Parquet file\n",
    "final_eval_df = pd.read_parquet(file_path, engine=\"pyarrow\")\n",
    "\n",
    "print(final_eval_df.dtypes)"
   ],
   "id": "4212634d2bbd25c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
